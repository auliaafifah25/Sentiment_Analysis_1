# -*- coding: utf-8 -*-
"""Proyek pertama: Predictive analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qSv0W6DaRSYp9KI1kJtUykrTlo2dZRLp

**Submission Pertama**
- **Nama:** Aulia Afifah
- **Email:** auliaafifah2205@gmail.com
- **ID Dicoding:** auliaafifah253

# Membaca data

mengimpor semua library yang dibutuhkan
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd

# %matplotlib inline
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

"""mempersiapkan data"""

df = pd.read_csv('review_dana_labelled.csv')
df.head()

"""# Analisis data

Melakukan tahap analisis data untuk memahami struktur dan distribusi data seperti akan mengecek informasi, emnghapus kolom, berapa jumlah data_train dan data_test, komposisi label atau target sentimen pada data, menggunakan fungsi world cloud untuk melihat term yang muncul pada data, dan lainnya.
"""

df.drop(columns=['userName', 'score', 'at'], axis=1, inplace=True)
df.head()

df.describe()

df.info()

"""cek jumlah data pada data"""

print(df.shape)

"""menghitung jumlah masing-masing variabel (positif, netral, dan negatif) pada kolom sentimen"""

df['sentimen'].value_counts()

df.groupby('sentimen').size().plot(kind='bar')

"""**Pembagian data**

Setelah melakukan analisis awal pada data, langkah berikutnya adalah membagi data menjadi data latih (train) dan data uji (test). 20% dari data akan digunakan sebagai data uji dan 80% sebagai data latih.
"""

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Data train sebesar: {train_df.shape}")
print(f"Data test sebesar: {test_df.shape}")

"""Data Latih (Train Data): Digunakan untuk melatih model machine learning."""

train_df.head()

"""Data Uji (Test Data): Digunakan untuk menguji dan mengevaluasi kinerja model."""

test_df.head()

"""melihat panjang teks dalam data train dan test"""

length_train = train_df['content'].str.len()
length_test = test_df['content'].str.len()
plt.figure(figsize=(10,6))
plt.hist(length_train, bins=50, label="Train_content", color = "darkblue")
plt.hist(length_test, bins=50, label='Test_content', color = "skyblue")
plt.legend()

"""kata-kata yang paling sering muncul dalam dokumen"""

#sentimen yang ada pada seluruh data train dapat dilihat dengan memahami kata yang umum dengan plot word cloud

def wordCloud(words):
    wordCloud = WordCloud(width=800, height=500, background_color='white', random_state=21, max_font_size=120).generate(words)

    plt.figure(figsize=(10, 7))
    plt.imshow(wordCloud, interpolation='bilinear')
    plt.axis('off')

all_words = ' '.join([text for text in train_df['content']])
wordCloud(all_words)

#melihat kata-kata yang positif
positive_words = ' '.join(text for text in train_df['content'][train_df['sentimen'] == 'POSITIVE'])
wordCloud(positive_words)

#melihat kata-kata yang negatif
negative_words = ' '.join(text for text in train_df['content'][train_df['sentimen'] == 'NEGATIVE'])
wordCloud(negative_words)

#melihat kata-kata yang neutral
neutral_words = ' '.join(text for text in train_df['content'][train_df['sentimen'] == 'NEUTRAL'])
wordCloud(neutral_words)

"""Melakukan penghilangan kata umum (stop words) yang tidak memberikan banyak informasi untuk analisis."""

stop_words_indonesia = [
    'yang', 'dan', 'di', 'ke', 'dari', 'untuk', 'dengan', 'pada', 'sebagai', 'adalah',
    'ini', 'itu', 'atau', 'saya', 'kamu', 'dia', 'kami', 'mereka', 'nya', 'yg', 'aja'
]

vectorizer_count = CountVectorizer(stop_words=stop_words_indonesia)

positive_counts = vectorizer_count.fit_transform(df[df['sentimen'] == 'POSITIVE']['content'])
positive_word_counts = pd.DataFrame(positive_counts.sum(axis=0), columns=vectorizer_count.get_feature_names_out()).T
positive_word_counts.columns = ['frequency']
positive_word_counts = positive_word_counts.sort_values(by='frequency', ascending=False)

print("Top 10 kata dalam sentimen positif:")
print(positive_word_counts.head(10))

negative_counts = vectorizer_count.fit_transform(df[df['sentimen'] == 'NEGATIVE']['content'])
negative_word_counts = pd.DataFrame(negative_counts.sum(axis=0), columns=vectorizer_count.get_feature_names_out()).T
negative_word_counts.columns = ['frequency']
negative_word_counts = negative_word_counts.sort_values(by='frequency', ascending=False)

print("Top 10 kata dalam sentimen negatif:")
print(negative_word_counts.head(10))

""" Mengekstrak n-grams dari teks untuk analisis lebih lanjut"""

def get_ngrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq

positive_bigrams = get_ngrams(df[df['sentimen'] == 'POSITIVE']['content'], n=3)
print("Top 10 bigram dalam sentimen positif:")
print(positive_bigrams[:10])

negative_bigrams = get_ngrams(df[df['sentimen'] == 'NEGATIVE']['content'], n=3)
print("Top 10 bigram dalam sentimen negatif:")
print(negative_bigrams[:10])

"""*   Bigram dalam Sentimen Positif: Berdasarkan frekuensi, kita melihat bahwa bigram "sangat membantu sekali" memiliki frekuensi tertinggi dengan 208 kemunculan, diikuti oleh "sangat bagus dan" dan "sangat membantu dan". Mayoritas bigram dalam sentimen positif menyoroti kepuasan pengguna terhadap layanan DANA, dengan menekankan kata-kata positif seperti "membantu", "bagus", dan "mudah".
*   Bigram dalam Sentimen Negatif: Dalam sentimen negatif, bigram yang paling umum adalah "akun dana saya" dengan frekuensi tertinggi 268 kemunculan, diikuti oleh "saya tidak bisa" dan "upgrade ke premium". Bigram ini menunjukkan adanya masalah atau keluhan yang berkaitan dengan pengalaman pengguna yang kurang memuaskan, seperti masalah teknis ("tidak bisa") atau masalah dengan upgrade ke premium.

# Feature Engineering dengan TF-IDF

menggunakan fungsi TfidfVectorizer dari library scikit-learn
"""

#melakukan vektorisasi untuk mengekstrak fitur dengan TF-IDF
vectorizer = TfidfVectorizer(stop_words=stop_words_indonesia,
                             min_df = 5,
                             max_df = 0.8,
                             sublinear_tf = True,
                             use_idf = True)

train_vectors = vectorizer.fit_transform(train_df['content'])
test_vectors = vectorizer.transform(test_df['content'])

"""# Klasifikasi Sentimen

## Support Vector Machine
"""

# melakukan klasifikasi dengan SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
classifier_linear.fit(train_vectors, train_df['sentimen'])
prediction_linear = classifier_linear.predict(test_vectors)

# print metriks
sentimen_names = ['POSITIVE', 'NEGATIVE', 'NEUTRAL']
print(classification_report(test_df['sentimen'], prediction_linear, target_names=sentimen_names))

"""Prediksi sentimen"""

content = """saya suka banget pakai dana, i love u"""
content_vector = vectorizer.transform([content])
print(classifier_linear.predict(content_vector))

content = """saya mau transfer, tapi ga bisa terus, gimana sih agak kecewa"""
content_vector = vectorizer.transform([content])
print(classifier_linear.predict(content_vector))

"""## Random Forest"""

# melakukan klasifikasi dengan Random Forest
classifier_rf = RandomForestClassifier(n_estimators=100, random_state=42)
classifier_rf.fit(train_vectors, train_df['sentimen'])
prediction_rf = classifier_rf.predict(test_vectors)

# print metriks
sentimen_names = ['Positive', 'Negative', 'Neutral']
print(classification_report(test_df['sentimen'], prediction_rf, target_names=sentimen_names))

"""Prediksi sentimen"""

content = """saya mau transfer, tapi ga bisa terus, gimana sih agak kecewa"""
content_vector = vectorizer.transform([content])
print(classifier_rf.predict(content_vector))

content = """saya suka banget pakai dana, i love u"""
content_vector = vectorizer.transform([content])
print(classifier_rf.predict(content_vector))